<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Solution Pattern: Name Template :: Solution Patterns for Cloud Native Architectures</title>
    <link rel="canonical" href="https://redhat-solution-patterns.github.io/solution-patterns/solution-pattern-edge-to-core-pipelines/02-architecture.html">
    <link rel="prev" href="index.html">
    <link rel="next" href="03-demo.html">
    <meta name="generator" content="Antora 3.0.0">
    <link rel="stylesheet" href="../_/css/site.css">
<link rel="icon" href="../_/img/favicon.ico" type="image/x-icon">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  </head>
  <body class="article">
<!-- Google tag (gtag.js) -->

<!--
  replace this with Google tag content
-->

<!-- Google tag (gtag.js) -->
<header class="header">

  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" style="font-size: 20px; color: white"
        href="https://redhat-solution-patterns.github.io/solution-patterns">
        <img src="../_/img/logo.png" height="35px" alt="Solution Patterns">&nbsp; Solution Patterns for Cloud Native Architectures</a>
      <button class="navbar-burger" data-target="topbar-nav">
      </button>
    </div>

    <div class="navbar-item"><span class="navbar-text"
        style="color: white; margin-left: 1rem; margin-right: 1rem;">|</span></div>

    <div class="navbar-item has-dropdown is-hoverable">
      <a class="navbar-link" href="#" style="color: white">Architectures &amp; Patterns</a>
      <div class="navbar-dropdown">
        <a class="navbar-item" href="https://www.redhat.com/architect/portfolio/" target="_blank">Portfolio Architecture&nbsp; </a>
        <a class="navbar-item" href="https://redhat-solution-patterns.github.io/solution-patterns/patterns.html" target="_blank">Solution Patterns </a>
        <a class="navbar-item" href="https://validatedpatterns.io/" target="_blank">Validated Patterns&nbsp;</a>
        <a class="navbar-item" href="https://catalog.redhat.com/solutions" target="_blank">Ecosystem Solutions&nbsp;</a>
      </div>
    </div>

    <div class="navbar-item has-dropdown is-hoverable">
      <a class="navbar-link" href="#" style="color: white">Resources</a>
      <div class="navbar-dropdown">
        <a class="navbar-item" target="_blank" href="https://developers.redhat.com">Red Hat Developers</a>
        <a class="navbar-item" target="_blank" href="https://developers.redhat.com/app-dev-platform">App Dev Platform</a>
        <a class="navbar-item" target="_blank" href="https://www.redhat.com/en/products/application-foundations">Red Hat Application Services</a>
      </div>
    </div>



    <a class="navbar-item" style="color: white" target="_blank" href="https://github.com/redhat-solution-patterns/redhat-solution-patterns.github.io/issues">Feedback</a>

    <a class="navbar-item" style="color: white" target="_blank" href="https://redhat-solution-patterns.github.io/solution-patterns/contributors-guide.html">Contribute</a>

  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="solution-pattern-edge-to-core-pipelines" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html" class=" query-params-link">Edge-to-Core Pipelines: Gather and prepare data for AI/ML</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">1. Home page</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="index.html#use-cases">1.1 Use cases</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="index.html#_the_story_behind_this_solution_pattern">1.2 The story behind this solution pattern</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="index.html#_the_solution">1.3 The solution</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="index.html#_explore_more_solution_patterns">1.4 Explore more solution patterns</a>
  </li>
</ul>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="02-architecture.html">2. Architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#_common_challenges">1. Common challenges</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#tech_stack">2. Technology stack</a>
  </li>
  <li class="nav-item" data-depth="2">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="#in_depth">3. An in-depth look at the solution&#8217;s architecture</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="#_data_acquisition">3.1. Data Acquisition</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="#_data_preparation_and_modeling">3.2. Data Preparation</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="#_application_development_and_delivery">3.3. Application Development</a>
  </li>
  <li class="nav-item" data-depth="3">
    <a class="nav-link" href="#_edge_ml_inference">3.4. Edge ML Inference</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#more_tech">4. More about the technology stack</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="03-demo.html">3. See the Solution in Action</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="03-demo.html#_demonstration">1. Demonstration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="03-demo.html#_install_the_demonstration">2. Install the demonstration</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="03-demo.html#_walkthrough_guide">3. Walkthrough guide</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="https://redhat-solution-patterns.github.io/">4. Other Red Hat Solution Patterns</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Edge-to-Core Pipelines: Gather and prepare data for AI/ML</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <span class="title">Edge-to-Core Pipelines: Gather and prepare data for AI/ML</span>
      <ul class="versions">
        <li class="version is-current">
          <a href="index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">Edge-to-Core Pipelines: Gather and prepare data for AI/ML</a></li>
    <li><a href="02-architecture.html">2. Architecture</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="https://github.com/redhat-solution-patterns/solution-pattern-edge-to-cloud-pipelines/edit/main/documentation/modules/ROOT/pages/02-architecture.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<article class="doc">
<h1 class="page">Solution Pattern: Name Template</h1>
<h1 id="_architecture" class="sect0"><a class="anchor" href="#_architecture"></a><a class="link" href="#_architecture">Architecture</a></h1>
<div class="openblock partintro">
<div class="content">
Introduction for the architecture of this solution pattern.
</div>
</div>
<div class="sect1">
<h2 id="_common_challenges"><a class="anchor" href="#_common_challenges"></a><a class="link" href="#_common_challenges">1. Common Challenges</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>To better explain and detail the reasons for the existence of this solution pattern we’ll picture some common needs and challenges amongst organizations that already have production systems and seeks innovation and modernization.</p>
</div>
<div class="paragraph">
<p>AI/ML production deployment is an iterative process that goes beyond simply generating AI/ML models or even ML Ops.</p>
</div>
<div class="paragraph">
<p>The primary milestones in the AI/ML life cycle depending on an organization&#8217;s business goals for its AI/ML effort are as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Collect and prepare the data needed for your AI/ML project. It is critical to be astute in terms of what functions take place at the edge. We do not want to do compute or storage intensive operations at the Edge, but rather should use the core or cloud.
What we should concentrate on at the edge is data collecting and data inference, where you will use AIML to make real-time decisions.</p>
</li>
<li>
<p>Create ML models based on business objectives. MLOps should take place on the core or in the cloud to retrain models as needed.</p>
</li>
<li>
<p>Integrate the ML models with the intelligent applications that you will create to serve the model and assist in making critical business choices.</p>
</li>
<li>
<p>Model accuracy should be monitored and managed throughout time.</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/02-AIML-lifecycle.png" alt="02 AIML lifecycle" width="80%">
</div>
<div class="title">Figure 1. AI/ML lifecycle for the Edge.</div>
</div>
<div class="paragraph">
<p>As organizations continue to grapple with massive amounts of data being generated from sources ranging from device edge to off-site facilities and public and private cloud environments, data managers are encountering a new challenge: how to properly ingest and process that data to receive actionable intelligence in a timely manner.</p>
</div>
<div class="paragraph">
<p>With fresh, relevant data, businesses can learn effectively and adapt to changing customer behavior. However, managing vast amounts of ingested data  and preparing to make that data ready as soon as possible—preferably in real time—for analytics and artificial intelligence and machine learning (AI/ML), is extremely challenging for data engineers.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="tech_stack"><a class="anchor" href="#tech_stack"></a><a class="link" href="#tech_stack">2. Technology Stack</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>Red Hat® OpenShift® Container Platform, Red Hat OpenShift Data Foundation, Red Hat Application Foundations, and other tools can help automate the workflow of data ingestion, preparation, and management building data pipelines for hybrid cloud deployments that automate data processing upon acquisition.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://www.redhat.com/en/technologies/device-edge">Red Hat Device Edge</a></p>
</li>
<li>
<p><a href="https://www.redhat.com/en/products/application-foundations" target="_blank" rel="noopener">Red Hat Application Foundations</a></p>
<div class="ulist">
<ul>
<li>
<p><strong><a href="https://access.redhat.com/products/red-hat-amq#broker" target="_blank" rel="noopener">AMQ Broker</a>:</strong> Pure-Java multi-protocol message broker. It’s built on an efficient, asynchronous core with a fast native journal for message persistence and the option of shared-nothing state replication for high availability.</p>
</li>
<li>
<p><strong><a href="https://access.redhat.com/products/red-hat-amq#streams" target="_blank" rel="noopener">AMQ Streams</a>:</strong> Based on the Apache Kafka project, AMQ Streams offers a distributed backbone that allows microservices and other applications to share data with extremely high throughput and extremely low latency.</p>
</li>
<li>
<p><strong><a href="https://developers.redhat.com/products/redhat-build-of-apache-camel" target="_blank" rel="noopener">Red Hat build of Apache Camel</a>:</strong> Utilize the integration capabilities of Apache Camel and its vast component library in the Quarkus runtime, optimizing for peak application performance with fast start up time.</p>
</li>
<li>
<p><strong><a href="https://access.redhat.com/products/quarkus" target="_blank" rel="noopener">Red Hat build of Quarkus</a>:</strong> Utilizes an innovative compile-time boot process that moves typical runtime steps to compile time. The result is an application that can consume as little as 10’s of MB of memory and start in 10’s of milliseconds.</p>
</li>
</ul>
</div>
</li>
<li>
<p><a href="https://www.redhat.com/en/technologies/cloud-computing/openshift" target="_blank" rel="noopener">Red Hat OpenShift</a></p>
</li>
<li>
<p><a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-ai">Red Hat OpenShift AI</a></p>
</li>
<li>
<p><a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation">Red Hat OpenShift Data Foundation</a></p>
</li>
<li>
<p><a href="https://www.redhat.com/en/technologies/cloud-computing/service-interconnect">Red Hat Service Interconnect</a></p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="in_depth"><a class="anchor" href="#in_depth"></a><a class="link" href="#in_depth">3. An in-depth look at the solution&#8217;s architecture</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>This solution pattern implements an architecture that allows large organisations to have a core data center doing all the CPU-hungry processing (AI/ML training) where different branches of the company are connected to push training data and obtain upgraded model versions.</p>
</div>
<div class="paragraph">
<p>In the retail space for instance you may find a company that runs multiple stores with different needs. One branch (<em>Edge1</em> for example) runs a big surface area store, with a big product catalogue aimed at the every-day citizen. Another branch (<em>Edge2</em>), small-sized store, may be located in a prominent area and offers luxury products to their customers. Both require different product catalogues and run at different speeds.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/07-full-architecture.png" alt="07 full architecture" width="80%">
</div>
</div>
<div class="paragraph">
<p>Each <em>Near Edge</em> zone in the diagram above represents the branches (stores) and run independent from each other. They all rely on the main data center to produce iterations of the AI/ML models they run for their stores.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The demo provided in this Solution Pattern can be provisioned with multiple edge zones, as per the diagram above, to showcase zone isolation and scalability (growing number of branches).</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In the sections that follow the focus is centred in the interactions between one of near-edge zones (one single branch) and the core data center.</p>
</div>
<div class="sect2">
<h3 id="_data_acquisition"><a class="anchor" href="#_data_acquisition"></a><a class="link" href="#_data_acquisition">3.1. Data Acquisition</a></h3>
<div class="paragraph">
<p>Ingesting and preparing data help organizations to deploy automated data pipelines in an event-driven architecture. In other spaces where continuous data is produced, such as sensor metrics, temperature, and vibrations can be imported directly into an Apache Kafka topic in AMQ Streams for example.</p>
</div>
<div class="paragraph">
<p>Data engineers can create pipelines to help automate workflows and business processes.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Discrete data:</strong> Images, video, documents, are types of content data you can store in object buckets.</p>
</li>
<li>
<p><strong>Multiprotocol:</strong> Data can be acquired using different protocols. Databases can connect to Kafka through a Debezium connector for example.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The <em>Data Acquisition</em> stage in the solution pattern is presented in the diagram below:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/08-stage1-data-aquisition.png" alt="08 stage1 data aquisition">
</div>
</div>
<div class="paragraph">
<p>When a new product needs to be added in the catalogue, its imagery is pushed to the system (<em>Device</em> to <em>Edge1</em>). The data is kept in local object storage while waiting to complete the collection. When the imagery is collected in full, an integration application (<em>Camel</em>) packages all the data and sends it over the wire to a remote S3 bucket located in the main data center (<em>Central</em>).</p>
</div>
<div class="paragraph">
<p>When the training data upload to S3 completes, a coordination signal is sent via Kafka to announce new training data is available, allowing</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Edge/Core connectivity is enabled with Service Interconnect. More details to follow.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><br></p>
</div>
</div>
<div class="sect2">
<h3 id="_data_preparation_modelling"><a class="anchor" href="#_data_preparation_modelling"></a><a class="link" href="#_data_preparation_modelling">3.2. Data Preparation &amp; Modelling</a></h3>
<div class="paragraph">
<p>The second stage is focussed on two important tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Design and Implement the AI/ML model that meets the business requirements.</p>
</li>
<li>
<p>Prepare/Pre-process the raw data to be used as input training data.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There is considerable manual labour involved into the activities above listed. However, once the team of data scientists produce the desired implementation to generate the AI/ML models, it can be injected as part of an automated pipeline, along with other automated steps.</p>
</div>
<div class="paragraph">
<p>The same is true for the pre-processing work to refine the raw training data which can be added to the chain of automated steps in the pipeline.</p>
</div>
<div class="paragraph">
<p>The solution pattern implements this stage as a fully automated workflow, initiated by the Kafka signal sent by the near edge system as illustrated in the diagram below:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/09-stage2-data-prep-n-model.png" alt="09 stage2 data prep n model">
</div>
</div>
<div class="paragraph">
<p>The Kafka (<em>Streams</em>) event (signal) announces new training data is available. The <em>Camel</em> integration consumes the event and triggers the automated pipeline.</p>
</div>
<div class="paragraph">
<p>The pipeline obtains all the training data from the S3 bucket and starts training the new AI/ML model. When the training completes, it uploads the new version to a model repository.</p>
</div>
</div>
<div class="sect2">
<h3 id="_application_development_and_delivery"><a class="anchor" href="#_application_development_and_delivery"></a><a class="link" href="#_application_development_and_delivery">3.3. Application Development and Delivery</a></h3>
<div class="paragraph">
<p>The third stage is primarily focussed on including all the <a href="https://www.redhat.com/en/topics/devops"><em>DevOps</em></a> and <a href="https://www.redhat.com/en/topics/ai/what-is-mlops"><em>MLOps</em></a> processes necessary to deliver the applications and upgrades the near edge needs to deploy and run.</p>
</div>
<div class="paragraph">
<p>Here, all the best cloud-native practices apply. To know more, follow the link of the resources listed below:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://developers.redhat.com/e-books/devops-culture-and-practice-openshift">DevOps Culture and Practice with OpenShift</a></p>
</li>
<li>
<p><a href="https://www.redhat.com/en/resources/mlops-architecture-openshift-infographic">MLOps: Machine learning operations with Red Hat OpenShift</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Smart applications, powered by the AI/ML models, need to be designed and implemented along with their automated delivery mechanisms.</p>
</div>
<div class="paragraph">
<p>The demo provided in this <em>Solution Pattern</em> already includes all the applications and integration systems. The focus is set on showcasing the automated pipeline responsible for producing new model versions which are uploaded to the platform&#8217;s object storage system, keeping duplicates in a model repository.</p>
</div>
<div class="paragraph">
<p>The diagram below illustrates the stage:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/10-stage3-app-dev-delivery.png" alt="10 stage3 app dev delivery">
</div>
</div>
<div class="paragraph">
<p>When the pipeline produces new model versions it pushes a copy into an edge-dedicated S3 bucket. An integration system (Camel) on the near edge environment monitors the bucket and when a new model version is available it downloads and hot-deploys it in the model server.</p>
</div>
</div>
<div class="sect2">
<h3 id="_edge_ml_inference"><a class="anchor" href="#_edge_ml_inference"></a><a class="link" href="#_edge_ml_inference">3.4. Edge ML Inference</a></h3>
<div class="paragraph">
<p>The last stage involves customers and users generating live traffic that interacts with the platform and triggers AI/ML inferences against the Model server.</p>
</div>
<div class="paragraph">
<p>Other systems are also at play in orchestrated workflows, executing business use cases and delivering responses to customers.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/11-stage4-ml-inferencing.png" alt="11 stage4 ml inferencing">
</div>
</div>
<div class="paragraph">
<p>The demo also provides a monitoring view that provides more in-depth insight into the systems that are involved end-to-end.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="more_tech"><a class="anchor" href="#more_tech"></a><a class="link" href="#more_tech">4. Additional notes about Edge Computing</a></h2>
<div class="sectionbody">
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Although not explicitly implemented in the Solution Pattern&#8217;s demonstration, the following sections discusses topics very relevant to our <em>Solution Pattern</em>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><a href="https://www.redhat.com/en/topics/edge-computing/what-is-edge-computing">Edge computing</a> shifts computing power away from core data-centers and distributes it closer to users and data sources—often across a large number of locations, providing faster response times, more reliable services, and a better application experience back to users.</p>
</div>
<div class="sect2">
<h3 id="_what_is_red_hat_device_edge"><a class="anchor" href="#_what_is_red_hat_device_edge"></a><a class="link" href="#_what_is_red_hat_device_edge">4.1. What is Red Hat Device Edge?</a></h3>
<div class="paragraph">
<p>Red Hat® Device Edge extends operational consistency across edge and hybrid cloud environments, no matter where devices are deployed in the field. Red Hat Device Edge combines enterprise-ready lightweight Kubernetes container orchestrations using MicroShift with Red Hat Enterprise Linux® to support different use cases and workloads on small, resource-constrained devices at the farthest edge.</p>
</div>
<div class="paragraph">
<p>MicroShift comes as an RPM software package that you can add to the blueprint of your system images when needed. Include your Kubernetes workloads, too, if you want. They will be deployed the next time you roll out updates to your devices. Red Hat Device Edge with MicroShift runs on Intel and Arm systems as small as 2 CPU cores and 2GB RAM.</p>
</div>
<div class="paragraph">
<p>MicroShift also provides OpenShift’s APIs for security context constraints and routes, but to reduce footprint we’ve removed APIs that are only useful on build clusters or clusters with multi-user interactive access. We’ve also removed Operators responsible for managing the operating system updates and configuration or orchestrating control plane components, as they are not needed in the MicroShift model.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/02-device-edge.png" alt="02 device edge" width="60%">
</div>
<div class="title">Figure 2. Red Hat Device Edge Technical Overview.</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Learn more about Red Hat Device Edge collaborations with <a href="https://www.redhat.com/en/about/press-releases/lockheed-martin-red-hat-collaborate-advance-artificial-intelligence-military-missions">Lockheed Martin</a> and <a href="https://www.redhat.com/en/about/press-releases/abb-and-red-hat-partner-deliver-further-scalable-digital-solutions-across-industrial-edge-and-hybrid-cloud">ABB</a>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p><br></p>
</div>
</div>
<div class="sect2">
<h3 id="_single_node_apache_kafka_broker"><a class="anchor" href="#_single_node_apache_kafka_broker"></a><a class="link" href="#_single_node_apache_kafka_broker">4.2. Single Node Apache Kafka Broker</a></h3>
<div class="paragraph">
<p>The Red Hat® AMQ streams component is a massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. It offers a distributed backbone that allows microservices and other applications to share data with high throughput and low latency.</p>
</div>
<div class="paragraph">
<p>The latest AMQ Streams release introduces the new <code>UseKRaft</code> feature gate. This feature gate provides a way to deploy a Kafka cluster in the KRaft (Kafka Raft metadata) mode without ZooKeeper. This feature gate is currently in an experimental stage, but it can be used for development and testing of AMQ Streams and Apache Kafka.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="_images/02-kafka-kraft-cluster.png" alt="02 kafka kraft cluster" width="60%">
</div>
<div class="title">Figure 3. KRaft architecture for Kafka..</div>
</div>
<div class="sidebarblock">
<div class="content">
<div class="paragraph">
<p>With KRaft, we can deploy a single node Kafka broker that also serves as the controller. All of the advantages of stream processing in a small footprint.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html" class="query-params-link">1. Home page</a></span>
  <span class="next"><a href="03-demo.html" class="query-params-link">3. See the Solution in Action</a></span>
</nav>
</article>
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
  </div>
</main>
</div>
<footer class="footer">
  <img
          src="../_/img/app-services-logo.png" height="40px" alt="Cloud Native Architecture Solution Patterns"  href="https://redhat.com" ></a>
</footer><script src="../_/js/vendor/clipboard.js"></script>
<script src="../_/js/site.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>
  </body>
</html>
